## Implementing transformer from scratch

### The following have been implemented:
 - Self attention
 - Causal attention
 - Multi head attention
 - GELU activation
 - Feed forward
 - Layer normalization
 - Skip connections

 ### TO DO
 - Train a small model on a dataset
 - Vectorize operations and remove redundancies/slow code
 - Add GPU acceleration support