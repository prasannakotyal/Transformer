## Implementing transformer from scratch

### The following have been implemented:
 - Self attention
 - Causal attention
 - Multi head attention
 - GELU activation
 - Feed forward
 - Layer normalization
 - Skip connections
 - Train it on shakespeare data

 ### TO DO
 - Vectorize operations and remove redundancies/slow code
 - Add GPU acceleration support

### Loss curve
![Loss Curve](https://github.com/user-attachments/assets/494261e8-d2ad-4a52-9ad0-1f9209bdf641)

### Sample Generated Output
![Image](https://github.com/user-attachments/assets/0648fa32-5c3f-499a-9120-275493ae33bc)
