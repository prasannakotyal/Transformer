{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "3zboiEXb-ydg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ezA0Jsk-4_v"
      },
      "outputs": [],
      "source": [
        "# Coding the self attention block\n",
        "'''\n",
        "Input: input embedding vector\n",
        "Parameters: d_in (number of input tokens), d_out (dimension of each input token), context_length (number of input tokens used to predict output)\n",
        "Output: context vector\n",
        "What happens: we create three weight matrices called query, key and value. We multiply\n",
        "these weight matrices with the input embedding vector (input + position).\n",
        "We then get attn scores (query*key.T) -> attn weights (scaling + softmax) -> context vector (attn weights*value)\n",
        "\n",
        "'''\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    queries = self.W_query(X) # shape (b,num_tokens,d_out)\n",
        "    keys = self.W_key(X) # shape (b,num_tokens,d_out)\n",
        "    values = self.W_value(X) # shape (b,num_tokens,d_out)\n",
        "\n",
        "\n",
        "    attn_scores = (queries@keys.T)/keys.shape[-1]**0.5 # normalize by sqrt(d_out of keys) because it helps reduce variance of softmax\n",
        "    attn_weights = torch.softmax(attn_scores,dim=1) #shape: (num_tokens,num_tokens)\n",
        "    context_vector = attn_weights@values # shape: (b,num_tokens,d_out)\n",
        "\n",
        "    return context_vector  # every row of the context vector corresponds to the context for that particular token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIbnxT3eECMu",
        "outputId": "fba88810-e09e-42ba-f662-2c7baa8133ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Sample:\n",
            "\n",
            "tensor([[-1.0915,  0.7671,  1.3740],\n",
            "        [ 1.2651,  1.5660,  0.5978],\n",
            "        [-0.2918, -1.0493, -0.0675],\n",
            "        [ 1.1399,  0.5300,  1.0625],\n",
            "        [ 0.9558, -0.4200, -2.1112]]) torch.Size([5, 3])\n",
            "\n",
            "\n",
            "Batched Input Sample:\n",
            "\n",
            "tensor([[[-1.0915,  0.7671,  1.3740],\n",
            "         [ 1.2651,  1.5660,  0.5978],\n",
            "         [-0.2918, -1.0493, -0.0675],\n",
            "         [ 1.1399,  0.5300,  1.0625],\n",
            "         [ 0.9558, -0.4200, -2.1112]],\n",
            "\n",
            "        [[-1.0915,  0.7671,  1.3740],\n",
            "         [ 1.2651,  1.5660,  0.5978],\n",
            "         [-0.2918, -1.0493, -0.0675],\n",
            "         [ 1.1399,  0.5300,  1.0625],\n",
            "         [ 0.9558, -0.4200, -2.1112]]]) torch.Size([2, 5, 3])\n"
          ]
        }
      ],
      "source": [
        "# Applying attention to an input example\n",
        "'''\n",
        "Input shape: [batch_size=2, seq_len=3, d_in=4]\n",
        "'''\n",
        "\n",
        "torch.manual_seed(34234)\n",
        "print(\"Input Sample:\\n\")\n",
        "X = torch.randn((5,3))\n",
        "print(X, X.shape)\n",
        "print(\"\\n\")\n",
        "print(\"Batched Input Sample:\\n\")\n",
        "batch = torch.stack((X,X),dim=0)\n",
        "print(batch,batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSA87q8oEN4Z",
        "outputId": "76087aa7-6bbb-490e-d4dc-19fefaec8bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context vector:\n",
            " tensor([[ 0.1478, -0.0381, -0.1513],\n",
            "        [ 0.1391,  0.0445, -0.0116],\n",
            "        [ 0.1680,  0.0959,  0.1104],\n",
            "        [ 0.1404,  0.0836,  0.0815],\n",
            "        [ 0.1921,  0.1566,  0.1930]], grad_fn=<MmBackward0>) torch.Size([5, 3])\n"
          ]
        }
      ],
      "source": [
        "d_in = batch.shape[-1]\n",
        "d_out = batch.shape[-1]\n",
        "attn = SelfAttention(d_in, d_out)\n",
        "context_vector = attn(X)\n",
        "\n",
        "print(\"Context vector:\\n\",context_vector,context_vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dyoa6aVzKVt"
      },
      "outputs": [],
      "source": [
        "# Coding the casual attention/ masked attention block\n",
        "\n",
        "'''\n",
        "Input: input embedding vector\n",
        "Parameters: d_in (number of input tokens), d_out (dimension of each input token), context_length (number of input tokens used to predict output)\n",
        "Output: context vector\n",
        "What happens: For any given output the inputs are the tokens that come before it.\n",
        "\n",
        "ex - hi, how are you?\n",
        "  hi -> how\n",
        "  hi how -> are\n",
        "  hi how are -> you\n",
        "  hi how are you => ?\n",
        "\n",
        "we do not have access to future tokens at all.\n",
        "The goal is to restrict the model to only consider the previous and current inputs in the sequence\n",
        "for a given token.\n",
        "Mask out all the tokens in the upper triangular matrix\n",
        "attention scores matrix:\n",
        "[\n",
        "  [a,-inf,-inf],\n",
        "  [a,b,-inf],\n",
        "  [a,b,c]\n",
        "]\n",
        "We add '-inf' as the mask because when we use softmax all the mask values become '0' (e^-inf).\n",
        "\n",
        "Steps:\n",
        "\n",
        "attention scores -> upper triangular infinity mask -> softmax\n",
        "\n",
        "'''\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) # causal attention mask which makes all elements in upper triangle '1'\n",
        "    self.register_buffer('mask', mask)\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    b,num_tokens,d_in = X.shape # new batch dimesnion b\n",
        "\n",
        "    queries = self.W_query(X) # shape (b,num_tokens,d_out)\n",
        "    keys = self.W_key(X) # shape (b,num_tokens,d_out)\n",
        "    values = self.W_value(X) # shape (b,num_tokens,d_out)\n",
        "\n",
        "    attn_scores = queries@keys.transpose(1,2) # initial keys shape: (b,num_tokens,d_out) after transposing last two dim, new shape: (b,d_out,num_tokens), attn_scores.shape: (b, num_tokens, num_tokens)\n",
        "    attn_scores.masked_fill_( # _ ops are in-place\n",
        "        self.mask.bool()[:num_tokens,:num_tokens],-torch.inf\n",
        "    ) # apply mask only for that particular sequence length/num tokens\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1) # normalize by sqrt(d_out of keys) because it helps reduce variance of softmax\n",
        "    attn_weights = self.dropout(attn_weights) # dropout layer -  drops %dropout from each layer. Used to prevent overfitting.\n",
        "    context_vector = attn_weights@values # every row of the context vector corresponds to the context for that particular token\n",
        "\n",
        "    return context_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcWebigx-R1J",
        "outputId": "7d6bf38f-3919-42aa-accc-b2d843f51379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Sample:\n",
            "\n",
            "tensor([[-1.0915,  0.7671,  1.3740],\n",
            "        [ 1.2651,  1.5660,  0.5978],\n",
            "        [-0.2918, -1.0493, -0.0675],\n",
            "        [ 1.1399,  0.5300,  1.0625],\n",
            "        [ 0.9558, -0.4200, -2.1112]]) torch.Size([5, 3])\n",
            "\n",
            "\n",
            "Batched Input Sample:\n",
            "\n",
            "tensor([[[-1.0915,  0.7671,  1.3740],\n",
            "         [ 1.2651,  1.5660,  0.5978],\n",
            "         [-0.2918, -1.0493, -0.0675],\n",
            "         [ 1.1399,  0.5300,  1.0625],\n",
            "         [ 0.9558, -0.4200, -2.1112]],\n",
            "\n",
            "        [[-1.0915,  0.7671,  1.3740],\n",
            "         [ 1.2651,  1.5660,  0.5978],\n",
            "         [-0.2918, -1.0493, -0.0675],\n",
            "         [ 1.1399,  0.5300,  1.0625],\n",
            "         [ 0.9558, -0.4200, -2.1112]]]) torch.Size([2, 5, 3])\n"
          ]
        }
      ],
      "source": [
        "# Applying causal attention to an input example\n",
        "\n",
        "torch.manual_seed(34234)\n",
        "print(\"Input Sample:\\n\")\n",
        "X = torch.randn((5,3))\n",
        "print(X, X.shape)\n",
        "print(\"\\n\")\n",
        "print(\"Batched Input Sample:\\n\")\n",
        "batch = torch.stack((X,X),dim=0)\n",
        "print(batch,batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u6oJOktA4KC",
        "outputId": "a95ee759-e3f7-4fb7-c546-5d604ebf040e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Vector:\n",
            " tensor([[[-1.1218,  0.8028, -0.7146,  0.6664],\n",
            "         [-0.4846,  0.1710, -0.2459,  0.9051],\n",
            "         [-0.5040,  0.3261, -0.3307,  0.5079],\n",
            "         [-0.2419,  0.1548, -0.2198,  0.4937],\n",
            "         [-0.2158,  0.1187, -0.1569,  0.3810]],\n",
            "\n",
            "        [[-1.1218,  0.8028, -0.7146,  0.6664],\n",
            "         [-0.4846,  0.1710, -0.2459,  0.9051],\n",
            "         [-0.5040,  0.3261, -0.3307,  0.5079],\n",
            "         [-0.2419,  0.1548, -0.2198,  0.4937],\n",
            "         [-0.2158,  0.1187, -0.1569,  0.3810]]], grad_fn=<UnsafeViewBackward0>) torch.Size([2, 5, 4])\n"
          ]
        }
      ],
      "source": [
        "batch_size,context_length,d_in = batch.shape\n",
        "d_out= 4\n",
        "ca = CausalAttention(d_in,d_out,context_length,0.0)\n",
        "context_vector = ca(batch)\n",
        "\n",
        "print(\"Context Vector:\\n\", context_vector,context_vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-1aH4vfBtTv"
      },
      "outputs": [],
      "source": [
        "# Coding the multi head attention block\n",
        "\n",
        "'''\n",
        "Input: input embedding vector\n",
        "Parameters: d_in (number of input tokens), d_out (dimension of each input token)\n",
        "Output: multiple context vectors\n",
        "What happens: use more than one attention heads, head_dim = d_out/no of heads. This helps in capturing multiple\n",
        "perceptions of the same input. We get multiple context vectors from each attention head and finally combine them\n",
        "to form a single big context vector.\n",
        "\n",
        "'''\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False): #included a num_heads parameter\n",
        "    super().__init__()\n",
        "    assert (d_out%num_heads==0), \\\n",
        "    \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out//num_heads\n",
        "\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.output_proj = nn.Linear(d_out,d_out) # linear layer to combine the outputs from the different attention heads\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    mask = torch.triu(torch.ones(context_length,context_length),diagonal=1) # causal attention mask which makes all elements in upper triangle '1'\n",
        "    self.register_buffer('mask',mask)\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    b,num_tokens,d_in = X.shape # new batch dimesnion b\n",
        "\n",
        "    queries = self.W_query(X) # shape (b,num_tokens,d_out)\n",
        "    keys = self.W_key(X) # shape (b,num_tokens,d_out)\n",
        "    values = self.W_value(X) # shape (b,num_tokens,d_out)\n",
        "\n",
        "    # Implicitly split the matrix by adding a 'num_heads' dimension\n",
        "    # Unroll last dimension: (b,num_tokens,d_out) -> (b,num_heads,num_heads,head_dim)\n",
        "\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    # Transpose: We need to group by the number of heads instead of the number of tokens\n",
        "    # We do a transopose to achieve this: (b,num_tokens,num_heads,head_dim) -> (b,,num_heads,num_tokens,head_dim)\n",
        "\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attn_scores = queries@keys.transpose(2,3) # initial keys shape: (b,num_heads,num_tokens,head_dim) after transposing last two dim, new shape: (b,num_heads,head_dim,num_tokens),\n",
        "    # attn_scores.shape: (b, d_out,num_tokens, num_tokens)\n",
        "    attn_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens,:num_tokens],-torch.inf\n",
        "    ) # apply mask only for that particular sequence length/num tokens\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1) # normalize by sqrt(d_out of keys) because it helps reduce variance of softmax\n",
        "    attn_weights = self.dropout(attn_weights) # dropout layer -  drops %dropout from each layer. Used to prevent overfitting.\n",
        "\n",
        "    context_vector = (attn_weights@values).transpose(1,2) # group by tokens for each head so its easier the merge, shape: (b,num_tokens,num_heads,head_dim)\n",
        "    # combine heads, self.d_out = num_heads*head_dim\n",
        "    context_vector = context_vector.contiguous().view(b,num_tokens,self.d_out) #contigous makes the matrix 'C contiguous' meaning that rows are stored next to each other in memory\n",
        "    # link - 'https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch' - notes on contigous\n",
        "    context_vector = self.output_proj(context_vector) #optional projection\n",
        "\n",
        "    return context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_f3CPtnWCEh",
        "outputId": "e6bc0ff3-b3cd-4c0b-b513-06b33f0aa7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Sample:\n",
            "\n",
            "tensor([[ 1.0112e+00, -4.9833e-01, -4.9400e-01, -6.6797e-01, -3.9417e-02,\n",
            "         -5.2224e-01],\n",
            "        [-6.7874e-01, -6.8613e-01,  1.1078e+00, -1.5318e-01,  7.3355e-01,\n",
            "         -6.1861e-01],\n",
            "        [-1.6816e-01,  4.1421e-04, -4.5069e-01,  8.1781e-01, -2.5656e-02,\n",
            "          3.9588e-01],\n",
            "        [ 9.5480e-01, -5.9837e-01,  3.5129e-01,  5.7757e-01,  2.1269e-01,\n",
            "         -1.1841e+00],\n",
            "        [ 4.6373e-02, -2.6153e+00, -4.3345e-02, -1.0756e+00, -1.6027e+00,\n",
            "          3.0792e-01]]) torch.Size([5, 6])\n",
            "\n",
            "\n",
            "Batched Input Sample:\n",
            "\n",
            "tensor([[[ 1.0112e+00, -4.9833e-01, -4.9400e-01, -6.6797e-01, -3.9417e-02,\n",
            "          -5.2224e-01],\n",
            "         [-6.7874e-01, -6.8613e-01,  1.1078e+00, -1.5318e-01,  7.3355e-01,\n",
            "          -6.1861e-01],\n",
            "         [-1.6816e-01,  4.1421e-04, -4.5069e-01,  8.1781e-01, -2.5656e-02,\n",
            "           3.9588e-01],\n",
            "         [ 9.5480e-01, -5.9837e-01,  3.5129e-01,  5.7757e-01,  2.1269e-01,\n",
            "          -1.1841e+00],\n",
            "         [ 4.6373e-02, -2.6153e+00, -4.3345e-02, -1.0756e+00, -1.6027e+00,\n",
            "           3.0792e-01]],\n",
            "\n",
            "        [[ 1.0112e+00, -4.9833e-01, -4.9400e-01, -6.6797e-01, -3.9417e-02,\n",
            "          -5.2224e-01],\n",
            "         [-6.7874e-01, -6.8613e-01,  1.1078e+00, -1.5318e-01,  7.3355e-01,\n",
            "          -6.1861e-01],\n",
            "         [-1.6816e-01,  4.1421e-04, -4.5069e-01,  8.1781e-01, -2.5656e-02,\n",
            "           3.9588e-01],\n",
            "         [ 9.5480e-01, -5.9837e-01,  3.5129e-01,  5.7757e-01,  2.1269e-01,\n",
            "          -1.1841e+00],\n",
            "         [ 4.6373e-02, -2.6153e+00, -4.3345e-02, -1.0756e+00, -1.6027e+00,\n",
            "           3.0792e-01]]]) torch.Size([2, 5, 6])\n"
          ]
        }
      ],
      "source": [
        "# Applying attention to an input example\n",
        "'''\n",
        "Input shape: [batch_size=2, seq_len=3, d_in=4]\n",
        "'''\n",
        "\n",
        "torch.manual_seed(34234)\n",
        "print(\"Input Sample:\\n\")\n",
        "X = torch.randn((5,6))\n",
        "print(X, X.shape)\n",
        "print(\"\\n\")\n",
        "print(\"Batched Input Sample:\\n\")\n",
        "batch = torch.stack((X,X),dim=0)\n",
        "print(batch,batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3y6ZMntWCHB",
        "outputId": "9951972c-738c-45ce-9fae-bd84349e3f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Vector:\n",
            " tensor([[[-0.2299,  0.0491,  0.0877, -0.4266],\n",
            "         [-0.3895,  0.2767,  0.2569, -0.4146],\n",
            "         [-0.4020,  0.2992,  0.3061, -0.3834],\n",
            "         [-0.3664,  0.3511,  0.3741, -0.3293],\n",
            "         [-0.3619,  0.2452,  0.2486, -0.3982]],\n",
            "\n",
            "        [[-0.2299,  0.0491,  0.0877, -0.4266],\n",
            "         [-0.3895,  0.2767,  0.2569, -0.4146],\n",
            "         [-0.4020,  0.2992,  0.3061, -0.3834],\n",
            "         [-0.3664,  0.3511,  0.3741, -0.3293],\n",
            "         [-0.3619,  0.2452,  0.2486, -0.3982]]], grad_fn=<ViewBackward0>) torch.Size([2, 5, 4])\n"
          ]
        }
      ],
      "source": [
        "batch_shape,context_length,d_in  = batch.shape\n",
        "d_out = 4\n",
        "num_heads = 2\n",
        "mha = MultiHeadAttention(d_in,d_out,context_length,0.0,num_heads)\n",
        "context_vector = mha(batch)\n",
        "\n",
        "print(\"Context Vector:\\n\", context_vector,context_vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB6lx2HQWCJZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p09X1eLtWCLh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
