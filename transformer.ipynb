{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F"
      ],
      "metadata": {
        "id": "1S-x7Xf2yNyV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z_R-yEFQo1uS"
      },
      "outputs": [],
      "source": [
        "# Transformer block implementation\n",
        "\n",
        "'''\n",
        "Input: Input embedding vector\n",
        "Parameters:\n",
        "Output:\n",
        "Note - dimensions of input is preserved after the trasnformre block operations.\n",
        "'''\n",
        "GPT_CFG_124M = {\n",
        "    'vocab_size': 50257,\n",
        "    'emb_dim': 768,\n",
        "    'context_length': 1024,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'drop_rate': 0.1,\n",
        "    'qkv_bias': False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False): #included a num_heads parameter\n",
        "    super().__init__()\n",
        "    assert (d_out%num_heads==0), \\\n",
        "    \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out//num_heads\n",
        "\n",
        "    self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.output_proj = nn.Linear(d_out,d_out) # linear layer to combine the outputs from the different attention heads\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    mask = torch.triu(torch.ones(context_length,context_length),diagonal=1) # causal attention mask which makes all elements in upper triangle '1'\n",
        "    self.register_buffer('mask',mask)\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "    b,num_tokens,d_in = X.shape # new batch dimesnion b\n",
        "\n",
        "    queries = self.W_query(X) # shape (b,num_tokens,d_out)\n",
        "    keys = self.W_key(X) # shape (b,num_tokens,d_out)\n",
        "    values = self.W_value(X) # shape (b,num_tokens,d_out)\n",
        "\n",
        "    # Implicitly split the matrix by adding a 'num_heads' dimension\n",
        "    # Unroll last dimension: (b,num_tokens,d_out) -> (b,num_heads,num_heads,head_dim)\n",
        "\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    # Transpose: We need to group by the number of heads instead of the number of tokens\n",
        "    # We do a transopose to achieve this: (b,num_tokens,num_heads,head_dim) -> (b,,num_heads,num_tokens,head_dim)\n",
        "\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attn_scores = queries@keys.transpose(2,3) # initial keys shape: (b,num_heads,num_tokens,head_dim) after transposing last two dim, new shape: (b,num_heads,head_dim,num_tokens),\n",
        "    # attn_scores.shape: (b, d_out,num_tokens, num_tokens)\n",
        "    attn_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens,:num_tokens],-torch.inf\n",
        "    ) # apply mask only for that particular sequence length/num tokens\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1) # normalize by sqrt(d_out of keys) because it helps reduce variance of softmax\n",
        "    attn_weights = self.dropout(attn_weights) # dropout layer -  drops %dropout from each layer. Used to prevent overfitting.\n",
        "\n",
        "    context_vector = (attn_weights@values).transpose(1,2) # group by tokens for each head so its easier the merge, shape: (b,num_tokens,num_heads,head_dim)\n",
        "    # combine heads, self.d_out = num_heads*head_dim\n",
        "    context_vector = context_vector.contiguous().view(b,num_tokens,self.d_out) #contigous makes the matrix 'C contiguous' meaning that rows are stored next to each other in memory\n",
        "    # link - 'https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch' - notes on contigous\n",
        "    context_vector = self.output_proj(context_vector) #optional projection\n",
        "\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "qGrdlkvAyKaV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5 # epsilon - used to avoid dividing by 0\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim)) # scale - trainable param\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim)) # shfit - trainable param\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdims=True)\n",
        "    var = x.var(dim=-1,keepdims=True,unbiased=False) # unbiased is for bessel's correction, True uses bessel's correction\n",
        "    x_norm = (x-mean)/torch.sqrt(var+self.eps) # standardization by adding epsilon\n",
        "    return self.scale*x_norm + self.shift"
      ],
      "metadata": {
        "id": "8Y_nS-U-zztX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
        "                               (x+0.044715*torch.pow(x,3))\n",
        "                               )) # phi(x) is approximated using this formula"
      ],
      "metadata": {
        "id": "bZrFTJNHzzwP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "BE7sKtnRzzy_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipConnections(nn.Module):\n",
        "  def __init__(self,layers_dim,use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    # Create Linear layers based on provided dimensions in layers_dim\n",
        "    self.layers = nn.ModuleList([\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layers_dim[0], layers_dim[1]),\n",
        "            GELU(),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layers_dim[1], layers_dim[0]),\n",
        "            GELU(),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layers_dim[2], layers_dim[3]),\n",
        "            GELU(),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layers_dim[3], layers_dim[4]),\n",
        "            GELU(),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(layers_dim[4], layers_dim[5]),\n",
        "            GELU(),\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      # calculate output\n",
        "      out = layer(x)\n",
        "      # check if shortcut can be applied\n",
        "      if self.use_shortcut and out.shape==x.shape:\n",
        "        x = x + out\n",
        "      # use if not shortcut can be applied\n",
        "      else:\n",
        "        x = out\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "8YmWPdqhzz1v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNormalization(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNormalization(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "P9XczoiCzz4P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing with a sample input\n",
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768) # [2 batches, 4 tokens per batch, 768 embedding dimension per token]\n",
        "block = TransformerBlock(GPT_CFG_124M)\n",
        "output = block(x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCRUFiGOzz7A",
        "outputId": "408a5c10-1d9b-4aea-ef08-90bb7da9eaed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    }
  ]
}